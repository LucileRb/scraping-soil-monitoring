{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scraping Soil Monitoring Data\n",
    "\n",
    "This notebook is designed to scrape search engine results related to soil monitoring, reporting, and verification (MRV). \n",
    "It retrieves and processes relevant URLs and domains to analyze the most frequently mentioned sources.\n",
    "\n",
    "#### Steps:\n",
    "1. Install and import required libraries.\n",
    "2. Define a web scraping function to collect search results.\n",
    "3. Process and clean the extracted data.\n",
    "4. Analyze the results (top domains, keyword presence, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!conda install selenium -y\n",
    "!conda install tldextract -y\n",
    "!conda install pandas -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UBRdvav8wwK"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import tldextract\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the name of the browser you are using. Choose 'chrome', 'firefox' or 'edge'\n",
    "browser = 'chrome'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `scraping_soil`\n",
    "\n",
    "The `scraping_soil` function is designed to scrape Google search results for a given set of search terms.  \n",
    "It collects and processes search results from multiple pages, extracting key information such as:  \n",
    "- Titles  \n",
    "- URLs  \n",
    "- Search result text snippets  \n",
    "- Page position  \n",
    "- Type of content (SEO vs. SEA)  \n",
    "\n",
    "**Usage:**  \n",
    "Call the function with a list of search terms and the number of pages to scrape. The function returns a pandas DataFrame containing the extracted data.  \n",
    "\n",
    "**Example:**  \n",
    "```python\n",
    "search_terms = [\"soil monitoring\", \"carbon sequestration\"]\n",
    "df_results = scrapator(search_terms, 3)\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_soil(search_terms, page_number):\n",
    "\n",
    "    \"\"\"\n",
    "    Scrapes Google SERP (Search Engine Results Pages) for given search terms.\n",
    "\n",
    "    Parameters:\n",
    "    search_terms (list): List of queries to search for.\n",
    "    page_number (int): Number of pages to scrape per search term.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing extracted URLs, titles, and metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(f'Program started at: {start_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    if browser == 'chrome':\n",
    "        options = webdriver.ChromeOptions()\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "    elif browser == 'firefox':\n",
    "        options = webdriver.FirefoxOptions()\n",
    "        driver = webdriver.Firefox(options=options)\n",
    "    elif browser == 'edge':\n",
    "        options = webdriver.EdgeOptions()\n",
    "        driver = webdriver.Edge(options=options)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported browser. Choose 'chrome', 'firefox', or 'edge'.\")\n",
    "\n",
    "    # options = webdriver.ChromeOptions()\n",
    "    # driver = webdriver.Chrome(options = options)\n",
    "\n",
    "    print('***** Scraping in progress *****')\n",
    "\n",
    "    for term in search_terms:\n",
    "        print(f'\\n**** Searching for term: {term} ****\\n')\n",
    "        driver.get('https://google.com')\n",
    "\n",
    "        # Accept cookies if prompted\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#L2AGLb.tHlp8d'))\n",
    "            ).click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        time.sleep(round(random.uniform(1, 10), 1))\n",
    "\n",
    "        # Locate and use search bar\n",
    "        search_box = driver.find_element(By.NAME, 'q')\n",
    "        search_box.clear()\n",
    "        search_box.send_keys(term)\n",
    "        time.sleep(1)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        time.sleep(round(random.uniform(1, 10), 1))\n",
    "\n",
    "        all_data = []  # Store extracted data\n",
    "\n",
    "        for page in range(1, page_number + 1):\n",
    "            print(f'*** Scraping page {page} ***')\n",
    "\n",
    "            # Wait for page load / solve captcha if needed\n",
    "            time.sleep(100)\n",
    "\n",
    "            articles_SEO = driver.find_elements(By.CLASS_NAME, 'MjjYud')\n",
    "            articles_SEA = driver.find_elements(By.CLASS_NAME, 'uEierd')\n",
    "\n",
    "            # Extract SEO articles\n",
    "            for position, article in enumerate(articles_SEO, start=1):\n",
    "                try:\n",
    "                    title = article.find_element(By.TAG_NAME, 'h3').text\n",
    "                    link = article.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                    content = article.text\n",
    "                    has_image = 'Yes' if article.find_elements(By.CLASS_NAME, 'Z26q7c.UK95Uc.Sth6v') else 'No'\n",
    "                    all_data.append([link, title, content, term, page, position, 'SEO', has_image])\n",
    "                except:\n",
    "                    continue  # Skip if element not found\n",
    "\n",
    "            # Extract SEA (paid ads) articles\n",
    "            for position, ad in enumerate(articles_SEA, start=1):\n",
    "                try:\n",
    "                    title = ad.find_element(By.CLASS_NAME, 'CCgQ5.vCa9Yd.QfkTvb.N8QANc.MUxGbd.v0nnCb').text\n",
    "                    link = ad.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                    content = ad.text\n",
    "                    has_image = 'Yes' if ad.find_elements(By.CLASS_NAME, 'g-img.ZGomKf') else 'No'\n",
    "                    all_data.append([link, title, content, term, page, position, 'SEA', has_image])\n",
    "                except:\n",
    "                    continue  # Skip if element not found\n",
    "\n",
    "            # Navigate to the next page\n",
    "            try:\n",
    "                next_button = driver.find_element(By.XPATH, \"//*[contains(text(),'Next')]\") # Adapt the word to the language\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "                next_button.click()\n",
    "                time.sleep(round(random.uniform(1, 10), 1))\n",
    "            except:\n",
    "                print(\"No more pages available.\")\n",
    "                break\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_term = pd.DataFrame(all_data, columns = ['url', 'title', 'content', 'search_term', 'page', 'position', 'type', 'image'])\n",
    "        df_term.dropna(subset=['title'], inplace = True)\n",
    "        df_term['company'] = df_term['url'].apply(lambda x: tldextract.extract(x).domain)\n",
    "        df_term['domain'] = df_term['url'].apply(lambda x: urlparse(x).netloc)\n",
    "\n",
    "        df = pd.concat([df, df_term], ignore_index = True)\n",
    "\n",
    "    df['Date'] = date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    driver.quit()\n",
    "    end_time = datetime.now()\n",
    "    print(f'Program completed at: {end_time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "    print(f'Duration: {end_time - start_time}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of search queries related to soil monitoring and verification\n",
    "queries = [\n",
    "    'Monitor* AND Report* AND Verif* AND MRV AND soil*',\n",
    "    #soil monitoring reporting verification'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Intervention During Scraping  \n",
    "\n",
    "This script uses Selenium to scrape Google search results, which involves opening a web browser.  \n",
    "During execution, manual intervention may be required in the following cases:  \n",
    "\n",
    "- **Google CAPTCHA:** If Google detects unusual activity, it may prompt a CAPTCHA challenge that needs to be solved manually.  \n",
    "- **Consent pop-ups:** The script attempts to close the Google cookie consent pop-up, but in some cases, manual confirmation may be needed.  \n",
    "- **Page navigation issues:** If the script gets stuck on a page, refreshing or clicking manually might be necessary.  \n",
    "\n",
    "âš  **Recommendation:** Run the script in a visible browser session and monitor its progress to handle any manual actions when required.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scraping\n",
    "df = scraping_soil(\n",
    "    search_terms = queries,\n",
    "    page_number = 3 # please select the number of pages you wish to scrape (20 pages)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'] = 'France' # change according your country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty rows from the DataFrame and reset index\n",
    "df = df.loc[df['url'] != 'vide']\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names of the DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 20 most frequently appearing domains in the dataset\n",
    "df['domain'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting Results\n",
    "\n",
    "After scraping and processing the search results, the data is saved as a CSV file.  \n",
    "The filename includes the current date (`YYYY-MM-DD`) to keep track of different scraping sessions.  \n",
    "This allows easy comparison of results across multiple executions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results to a CSV file\n",
    "# Please adapt the filename with the filepath of your choice.\n",
    "df.to_csv(f'scraping_results_{date.today().strftime(\"%Y-%m-%d\")}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
