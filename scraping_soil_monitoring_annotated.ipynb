{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scraping Soil Monitoring Data\n",
    "\n",
    "This notebook is designed to scrape search engine results related to soil monitoring, reporting, and verification (MRV). \n",
    "It retrieves and processes relevant URLs and domains to analyze the most frequently mentioned sources.\n",
    "\n",
    "#### Steps:\n",
    "1. Install and import required libraries.\n",
    "2. Define a web scraping function to collect search results.\n",
    "3. Process and clean the extracted data.\n",
    "4. Analyze the results (top domains, keyword presence, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5UBRdvav8wwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in ./.venv/lib/python3.9/site-packages (4.28.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in ./.venv/lib/python3.9/site-packages (from selenium) (2.3.0)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in ./.venv/lib/python3.9/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: trio~=0.17 in ./.venv/lib/python3.9/site-packages (from selenium) (0.28.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in ./.venv/lib/python3.9/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./.venv/lib/python3.9/site-packages (from selenium) (2024.12.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in ./.venv/lib/python3.9/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: sortedcontainers in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: outcome in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in ./.venv/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in ./.venv/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./.venv/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/lucilerabeau/code/LucileRb/scraping-soil-monitoring/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tldextract in ./.venv/lib/python3.9/site-packages (5.1.3)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from tldextract) (3.10)\n",
      "Requirement already satisfied: filelock>=3.0.8 in ./.venv/lib/python3.9/site-packages (from tldextract) (3.17.0)\n",
      "Requirement already satisfied: requests>=2.1.0 in ./.venv/lib/python3.9/site-packages (from tldextract) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in ./.venv/lib/python3.9/site-packages (from tldextract) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests>=2.1.0->tldextract) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.1.0->tldextract) (2.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/lucilerabeau/code/LucileRb/scraping-soil-monitoring/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Install required libraries\n",
    "!pip install tldextract  # Extract top-level domains from URLs\n",
    "\n",
    "import tldextract\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.28.1\n"
     ]
    }
   ],
   "source": [
    "print(webdriver.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selenium version : {webdriver.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `scraping_soil`\n",
    "\n",
    "The `scraping_soil` function is designed to scrape Google search results for a given set of search terms.  \n",
    "It collects and processes search results from multiple pages, extracting key information such as:  \n",
    "- Titles  \n",
    "- URLs  \n",
    "- Search result text snippets  \n",
    "- Page position  \n",
    "- Type of content (SEO vs. SEA)  \n",
    "\n",
    "**Usage:**  \n",
    "Call the function with a list of search terms and the number of pages to scrape. The function returns a pandas DataFrame containing the extracted data.  \n",
    "\n",
    "**Example:**  \n",
    "```python\n",
    "search_terms = [\"soil monitoring\", \"carbon sequestration\"]\n",
    "df_results = scrapator(search_terms, 3)\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_soil(search_terms, page_number):\n",
    "\n",
    "    \"\"\"\n",
    "    Scrapes Google SERP (Search Engine Results Pages) for given search terms.\n",
    "\n",
    "    Parameters:\n",
    "    search_terms (list): List of queries to search for.\n",
    "    page_number (int): Number of pages to scrape per search term.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing extracted URLs, titles, and metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(f'Program started at: {start_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options = options)\n",
    "\n",
    "    print('***** Scraping in progress *****')\n",
    "\n",
    "    for term in search_terms:\n",
    "        print(f'\\n**** Searching for term: {term} ****\\n')\n",
    "        driver.get('https://google.com')\n",
    "\n",
    "        # Accept cookies if prompted\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#L2AGLb.tHlp8d'))\n",
    "            ).click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        time.sleep(round(random.uniform(1, 10), 1))\n",
    "\n",
    "        # Locate and use search bar\n",
    "        search_box = driver.find_element(By.NAME, 'q')\n",
    "        search_box.clear()\n",
    "        search_box.send_keys(term)\n",
    "        time.sleep(1)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "        time.sleep(round(random.uniform(1, 10), 1))\n",
    "\n",
    "        all_data = []  # Store extracted data\n",
    "\n",
    "        for page in range(1, page_number + 1):\n",
    "            print(f'*** Scraping page {page} ***')\n",
    "\n",
    "            # Wait for page load / solve captcha if needed\n",
    "            time.sleep(100)\n",
    "\n",
    "            articles_SEO = driver.find_elements(By.CLASS_NAME, 'MjjYud')\n",
    "            articles_SEA = driver.find_elements(By.CLASS_NAME, 'uEierd')\n",
    "\n",
    "            # Extract SEO articles\n",
    "            for position, article in enumerate(articles_SEO, start=1):\n",
    "                try:\n",
    "                    title = article.find_element(By.TAG_NAME, 'h3').text\n",
    "                    link = article.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                    content = article.text\n",
    "                    has_image = 'Yes' if article.find_elements(By.CLASS_NAME, 'Z26q7c.UK95Uc.Sth6v') else 'No'\n",
    "                    all_data.append([link, title, content, term, page, position, 'SEO', has_image])\n",
    "                except:\n",
    "                    continue  # Skip if element not found\n",
    "\n",
    "            # Extract SEA (paid ads) articles\n",
    "            for position, ad in enumerate(articles_SEA, start=1):\n",
    "                try:\n",
    "                    title = ad.find_element(By.CLASS_NAME, 'CCgQ5.vCa9Yd.QfkTvb.N8QANc.MUxGbd.v0nnCb').text\n",
    "                    link = ad.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                    content = ad.text\n",
    "                    has_image = 'Yes' if ad.find_elements(By.CLASS_NAME, 'g-img.ZGomKf') else 'No'\n",
    "                    all_data.append([link, title, content, term, page, position, 'SEA', has_image])\n",
    "                except:\n",
    "                    continue  # Skip if element not found\n",
    "\n",
    "            # Navigate to the next page\n",
    "            try:\n",
    "                next_button = driver.find_element(By.XPATH, \"//*[contains(text(),'Suivant')]\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "                next_button.click()\n",
    "                time.sleep(round(random.uniform(1, 10), 1))\n",
    "            except:\n",
    "                print(\"No more pages available.\")\n",
    "                break\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_term = pd.DataFrame(all_data, columns = ['url', 'title', 'content', 'search_term', 'page', 'position', 'type', 'image'])\n",
    "        df_term.dropna(subset=['title'], inplace = True)\n",
    "        df_term['company'] = df_term['url'].apply(lambda x: tldextract.extract(x).domain)\n",
    "        df_term['domain'] = df_term['url'].apply(lambda x: urlparse(x).netloc)\n",
    "\n",
    "        df = pd.concat([df, df_term], ignore_index = True)\n",
    "\n",
    "    df['Date'] = date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    driver.quit()\n",
    "    end_time = datetime.now()\n",
    "    print(f'Program completed at: {end_time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "    print(f'Duration: {end_time - start_time}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of search queries related to soil monitoring and verification\n",
    "queries = [\n",
    "    'Monitor* AND Report* AND Verif* AND MRV AND soil*',\n",
    "    #soil monitoring reporting verification'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Intervention During Scraping  \n",
    "\n",
    "This script uses Selenium to scrape Google search results, which involves opening a web browser.  \n",
    "During execution, manual intervention may be required in the following cases:  \n",
    "\n",
    "- **Google CAPTCHA:** If Google detects unusual activity, it may prompt a CAPTCHA challenge that needs to be solved manually.  \n",
    "- **Consent pop-ups:** The script attempts to close the Google cookie consent pop-up, but in some cases, manual confirmation may be needed.  \n",
    "- **Page navigation issues:** If the script gets stuck on a page, refreshing or clicking manually might be necessary.  \n",
    "\n",
    "⚠ **Recommendation:** Run the script in a visible browser session and monitor its progress to handle any manual actions when required.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started at: 2025-06-10 10:35:31\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Scraping in progress *****\n",
      "\n",
      "**** Searching for term: Monitor* AND Report* AND Verif* AND MRV AND soil* ****\n",
      "\n",
      "*** Scraping page 1 ***\n",
      "*** Scraping page 2 ***\n",
      "*** Scraping page 3 ***\n",
      "*** Scraping page 4 ***\n",
      "*** Scraping page 5 ***\n",
      "*** Scraping page 6 ***\n",
      "*** Scraping page 7 ***\n",
      "*** Scraping page 8 ***\n",
      "*** Scraping page 9 ***\n",
      "*** Scraping page 10 ***\n",
      "Program completed at: 2025-06-10 10:53:25\n",
      "Duration: 0:17:54.697686\n"
     ]
    }
   ],
   "source": [
    "# Run scraping\n",
    "df = scraping_soil(\n",
    "    search_terms = queries,\n",
    "    page_number = 10 # please select the number of pages you wish to scrape\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty rows from the DataFrame and reset index\n",
    "df = df.loc[df['url'] != 'vide']\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'] = 'France'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>search_term</th>\n",
       "      <th>page</th>\n",
       "      <th>position</th>\n",
       "      <th>type</th>\n",
       "      <th>image</th>\n",
       "      <th>company</th>\n",
       "      <th>domain</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.horizon-europe.gouv.fr/monitoring-...</td>\n",
       "      <td>Monitoring, reporting and verification of soil...</td>\n",
       "      <td>Monitoring, reporting and verification of soil...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>horizon-europe</td>\n",
       "      <td>www.horizon-europe.gouv.fr</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cordis.europa.eu/programme/id/HORIZON_...</td>\n",
       "      <td>Monitoring, reporting and verification of soil...</td>\n",
       "      <td>Monitoring, reporting and verification of soil...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>europa</td>\n",
       "      <td>cordis.europa.eu</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://unece.org/sustainable-energy/monitorin...</td>\n",
       "      <td></td>\n",
       "      <td>Autres questions\\nWhat is mrv monitoring, repo...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>unece</td>\n",
       "      <td>unece.org</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://4p1000.org/mrv-systems-related-to-soil...</td>\n",
       "      <td>MRV systems related to Soil health in the cont...</td>\n",
       "      <td>MRV systems related to Soil health in the cont...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>4p1000</td>\n",
       "      <td>4p1000.org</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Solutions and insights for agricultural monito...</td>\n",
       "      <td>Solutions and insights for agricultural monito...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>sciencedirect</td>\n",
       "      <td>www.sciencedirect.com</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://cgspace.cgiar.org/items/c29a7507-13a3-...</td>\n",
       "      <td>Monitoring, reporting, and verification system...</td>\n",
       "      <td>Monitoring, reporting, and verification system...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>cgiar</td>\n",
       "      <td>cgspace.cgiar.org</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.icos-cp.eu/news-and-events/news/fl...</td>\n",
       "      <td>FLUXES, The European Greenhouse Gas Bulletin: MRV</td>\n",
       "      <td>FLUXES, The European Greenhouse Gas Bulletin: ...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>icos-cp</td>\n",
       "      <td>www.icos-cp.eu</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://journals.plos.org/plosone/article?id=1...</td>\n",
       "      <td>Moving from Measuring, Reporting, Verification...</td>\n",
       "      <td>Moving from Measuring, Reporting, Verification...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>plos</td>\n",
       "      <td>journals.plos.org</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.naro.go.jp/publicity_report/public...</td>\n",
       "      <td>Handbook of Monitoring, Reporting, and Verific...</td>\n",
       "      <td>Handbook of Monitoring, Reporting, and Verific...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>naro</td>\n",
       "      <td>www.naro.go.jp</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://soils.org.uk/wp-content/uploads/2023/0...</td>\n",
       "      <td>What are carbon stocks and how can they be mea...</td>\n",
       "      <td>What are carbon stocks and how can they be mea...</td>\n",
       "      <td>Monitor* AND Report* AND Verif* AND MRV AND soil*</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>SEO</td>\n",
       "      <td>No</td>\n",
       "      <td>soils</td>\n",
       "      <td>soils.org.uk</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0    https://www.horizon-europe.gouv.fr/monitoring-...   \n",
       "1    https://cordis.europa.eu/programme/id/HORIZON_...   \n",
       "2    https://unece.org/sustainable-energy/monitorin...   \n",
       "3    https://4p1000.org/mrv-systems-related-to-soil...   \n",
       "4    https://www.sciencedirect.com/science/article/...   \n",
       "..                                                 ...   \n",
       "97   https://cgspace.cgiar.org/items/c29a7507-13a3-...   \n",
       "98   https://www.icos-cp.eu/news-and-events/news/fl...   \n",
       "99   https://journals.plos.org/plosone/article?id=1...   \n",
       "100  https://www.naro.go.jp/publicity_report/public...   \n",
       "101  https://soils.org.uk/wp-content/uploads/2023/0...   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Monitoring, reporting and verification of soil...   \n",
       "1    Monitoring, reporting and verification of soil...   \n",
       "2                                                        \n",
       "3    MRV systems related to Soil health in the cont...   \n",
       "4    Solutions and insights for agricultural monito...   \n",
       "..                                                 ...   \n",
       "97   Monitoring, reporting, and verification system...   \n",
       "98   FLUXES, The European Greenhouse Gas Bulletin: MRV   \n",
       "99   Moving from Measuring, Reporting, Verification...   \n",
       "100  Handbook of Monitoring, Reporting, and Verific...   \n",
       "101  What are carbon stocks and how can they be mea...   \n",
       "\n",
       "                                               content  \\\n",
       "0    Monitoring, reporting and verification of soil...   \n",
       "1    Monitoring, reporting and verification of soil...   \n",
       "2    Autres questions\\nWhat is mrv monitoring, repo...   \n",
       "3    MRV systems related to Soil health in the cont...   \n",
       "4    Solutions and insights for agricultural monito...   \n",
       "..                                                 ...   \n",
       "97   Monitoring, reporting, and verification system...   \n",
       "98   FLUXES, The European Greenhouse Gas Bulletin: ...   \n",
       "99   Moving from Measuring, Reporting, Verification...   \n",
       "100  Handbook of Monitoring, Reporting, and Verific...   \n",
       "101  What are carbon stocks and how can they be mea...   \n",
       "\n",
       "                                           search_term  page  position type  \\\n",
       "0    Monitor* AND Report* AND Verif* AND MRV AND soil*     1         1  SEO   \n",
       "1    Monitor* AND Report* AND Verif* AND MRV AND soil*     1         3  SEO   \n",
       "2    Monitor* AND Report* AND Verif* AND MRV AND soil*     1         4  SEO   \n",
       "3    Monitor* AND Report* AND Verif* AND MRV AND soil*     1         5  SEO   \n",
       "4    Monitor* AND Report* AND Verif* AND MRV AND soil*     1         6  SEO   \n",
       "..                                                 ...   ...       ...  ...   \n",
       "97   Monitor* AND Report* AND Verif* AND MRV AND soil*    10         6  SEO   \n",
       "98   Monitor* AND Report* AND Verif* AND MRV AND soil*    10         7  SEO   \n",
       "99   Monitor* AND Report* AND Verif* AND MRV AND soil*    10         8  SEO   \n",
       "100  Monitor* AND Report* AND Verif* AND MRV AND soil*    10         9  SEO   \n",
       "101  Monitor* AND Report* AND Verif* AND MRV AND soil*    10        10  SEO   \n",
       "\n",
       "    image         company                      domain        Date Country  \n",
       "0      No  horizon-europe  www.horizon-europe.gouv.fr  2025-06-10  France  \n",
       "1      No          europa            cordis.europa.eu  2025-06-10  France  \n",
       "2      No           unece                   unece.org  2025-06-10  France  \n",
       "3      No          4p1000                  4p1000.org  2025-06-10  France  \n",
       "4      No   sciencedirect       www.sciencedirect.com  2025-06-10  France  \n",
       "..    ...             ...                         ...         ...     ...  \n",
       "97     No           cgiar           cgspace.cgiar.org  2025-06-10  France  \n",
       "98     No         icos-cp              www.icos-cp.eu  2025-06-10  France  \n",
       "99     No            plos           journals.plos.org  2025-06-10  France  \n",
       "100    No            naro              www.naro.go.jp  2025-06-10  France  \n",
       "101    No           soils                soils.org.uk  2025-06-10  France  \n",
       "\n",
       "[102 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'title', 'content', 'search_term', 'page', 'position', 'type',\n",
       "       'image', 'company', 'domain', 'Date', 'Country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the column names of the DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain\n",
       "www.icos-cp.eu                      3\n",
       "www.horizon-europe.gouv.fr          2\n",
       "openknowledge.fao.org               2\n",
       "www.researchgate.net                2\n",
       "research.wur.nl                     2\n",
       "www.academia.edu                    2\n",
       "pubmed.ncbi.nlm.nih.gov             2\n",
       "pureportal.ilvo.be                  2\n",
       "irc-orcasa.eu                       2\n",
       "hal.inrae.fr                        2\n",
       "unece.org                           2\n",
       "4p1000.org                          2\n",
       "www.un-redd.org                     2\n",
       "www.isric.org                       2\n",
       "www.sciencedirect.com               2\n",
       "www.deloitte.com                    2\n",
       "co2re.org                           1\n",
       "onlinelibrary.wiley.com             1\n",
       "ieaghg.org                          1\n",
       "assets.publishing.service.gov.uk    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the 20 most frequently appearing domains in the dataset\n",
    "df['domain'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting Results\n",
    "\n",
    "After scraping and processing the search results, the data is saved as a CSV file.  \n",
    "The filename includes the current date (`YYYY-MM-DD`) to keep track of different scraping sessions.  \n",
    "This allows easy comparison of results across multiple executions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results to a CSV file\n",
    "# Please adapt the filename with the filepath of your choice.\n",
    "df.to_csv(f'scraping_results_{date.today().strftime(\"%Y-%m-%d\")}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
